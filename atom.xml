<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[OSv Blog]]></title>
  <link href="http://osv.io/blog/atom.xml" rel="self"/>
  <link href="http://osv.io/blog/"/>
  <updated>2014-08-28T08:21:40-07:00</updated>
  <id>http://osv.io/blog/</id>
  <author>
    <name><![CDATA[Cloudius Systems]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Wiki Watch: Cloud Init]]></title>
    <link href="http://osv.io/blog/blog/2014/08/28/wiki-watch-cloud-init/"/>
    <updated>2014-08-28T08:37:05-07:00</updated>
    <id>http://osv.io/blog/blog/2014/08/28/wiki-watch-cloud-init</id>
    <content type="html"><![CDATA[<p><a href="http://osv.io/blog/blog/2014/08/26/jolokia-jmx-connectivity-in-osv/">The previous blog article, on Jolokia</a> mentions a configuration item for cloud-init.  You may be asking, &ldquo;Cloud init?  How do I use that on OSv?&rdquo;  Well, good news.  For details on configuring OSv using the cloud init mechanism, check out our recently updated wiki page: <a href="https://github.com/cloudius-systems/osv/wiki/cloud-init">Cloud init</a>.</p>

<p><a href="https://github.com/tgrabiec">Tomasz Grabiec</a> has written an informative page on how to configure your OSv instance, from setting the port for the management HTTP server to filling in the content of config files. Check it out.  (Naturally, it&rsquo;s a wiki, so feel free to fill in additional details if you like.)</p>

<p>Want more info on cloud-init and related topics?  Join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a>.  You can get regular OSv updates by subscribing to this blog&rsquo;s feed, or folllowing <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jolokia JMX Connectivity in OSv]]></title>
    <link href="http://osv.io/blog/blog/2014/08/26/jolokia-jmx-connectivity-in-osv/"/>
    <updated>2014-08-26T07:11:20-07:00</updated>
    <id>http://osv.io/blog/blog/2014/08/26/jolokia-jmx-connectivity-in-osv</id>
    <content type="html"><![CDATA[<p><strong>By Calle Wilund</strong></p>

<p>OSv is a great way to run Java applications in the cloud, and it recently became just a little bit better. As you are probably aware, OSv exposes quite a bit of information and manageability options through its <a href="https://github.com/cloudius-systems/osv/wiki/The-RESTful-API">RESTful API</a>, accessible through the built-in HTTP server. More or less from its inception, this API has exposed various aspects of the JVM and the <a href="http://docs.oracle.com/javase/7/docs/technotes/guides/jmx/">Java Management Beans</a> provided.</p>

<p>Recently we improved on this a bit by including the <a href="http://www.jolokia.org/">Jolokia</a> JMX-via-JSON-REST connector, providing full read/write access to the entire set of Java manageability attributes and operations. Now you no longer need to set up and secure separate JMX-over-RMI connectivity with your Java application to fully manage it.</p>

<p>The Jolokia API is available via the OSv REST server at <nobr><code>http[s]://&lt;OSv host&gt;:&lt;port&gt;/jolokia</code></nobr>. You can explore this and other API:s via the <a href="https://github.com/cloudius-systems/osv/wiki/The-RESTful-API#using-the-swagger-ui">Swagger UI</a>.
For a better understanding of the full <a href="http://www.jolokia.org/reference/html/protocol.html">Jolokia syntax</a>, I suggest reading through the <a href="http://www.jolokia.org/reference/html/index.html">reference manual</a>. In its simplest form, querying a Java Management Bean value from an OSv instance can be done like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; curl http://&lt;ip&gt;:&lt;port&gt;/jolokia/read/java.lang:type=Memory/HeapMemoryUsage</span></code></pre></td></tr></table></div></figure>


<p>With the result of something like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="p">{</span>   
</span><span class='line'>  <span class="s2">&quot;timestamp&quot;</span>   <span class="o">:</span><span class="mi">1409065190</span><span class="p">,</span>
</span><span class='line'>  <span class="s2">&quot;status&quot;</span>  <span class="o">:</span><span class="mi">200</span><span class="p">,</span>
</span><span class='line'>  <span class="s2">&quot;request&quot;</span> <span class="o">:</span> <span class="p">{</span>
</span><span class='line'>          <span class="s2">&quot;mbean&quot;</span>       <span class="o">:</span><span class="s2">&quot;java.lang:type=Memory&quot;</span><span class="p">,</span>
</span><span class='line'>          <span class="s2">&quot;attribute&quot;</span>   <span class="o">:</span><span class="s2">&quot;HeapMemoryUsage&quot;</span><span class="p">,</span>
</span><span class='line'>          <span class="s2">&quot;type&quot;</span>        <span class="o">:</span><span class="s2">&quot;read&quot;</span>
</span><span class='line'>          <span class="p">},</span>
</span><span class='line'>  <span class="s2">&quot;value&quot;</span>       <span class="o">:</span> <span class="p">{</span>
</span><span class='line'>          <span class="s2">&quot;max&quot;</span>     <span class="o">:</span><span class="mi">1839202304</span><span class="p">,</span>
</span><span class='line'>          <span class="s2">&quot;committed&quot;</span>   <span class="o">:</span><span class="mi">1839202304</span><span class="p">,</span>
</span><span class='line'>          <span class="s2">&quot;init&quot;</span>        <span class="o">:</span><span class="mi">1918894080</span><span class="p">,</span>
</span><span class='line'>          <span class="s2">&quot;used&quot;</span>        <span class="o">:</span><span class="mi">192117128</span>
</span><span class='line'>          <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Jolokia provides a full syntax for packaging JMX bean information inside JSON objects, including the ability to batch requests.  It also provides client connector libraries for <a href="http://www.jolokia.org/client/java.html">Java</a>, <a href="http://www.jolokia.org/client/perl.html">Perl</a> and <a href="http://www.jolokia.org/client/javascript.html">JavaScript</a> to access them easily from (web) applications.</p>

<h2>Important note about REST requests and browser security</h2>

<p>Most browsers today enforce that resources such as REST queries may only be made to the same domain as the requesting web page. When you want to allow <a href="http://www.w3.org/TR/cors/">cross-domain requests</a> you need to either turn off this security feature in your browser (for Google Chrome you can run it with <code>--disable-web-security</code>, however if you use Firefox I do not know of any way to do it), or enable CORS in OSv.</p>

<p>To do the latter, you need to provide a <code>httpserver</code> configuration section in your <a href="https://github.com/cloudius-systems/osv/wiki/Cloud-init">cloud init</a> settings. To simply allow all domains to make requests to the OSv APIs, add this to your configuration:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">httpserver</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">access-allow</span><span class="p-Indicator">:</span> <span class="s">&#39;*&#39;</span>
</span></code></pre></td></tr></table></div></figure>


<p>And make this reachable through your cloud-init server. (Or use the EC2 simulator script provided with OSv).</p>

<h2>A small demo</h2>

<p>A small demo JavaScript application showing how to easily plot various JVM graphs from a running OSv instance can be found at <a href="https://github.com/elcallio/jolokia-demo">https://github.com/elcallio/jolokia-demo</a> (which is a modified fork of the <a href="https://github.com/nurkiewicz/token-bucket">Jolokia demo</a> created by Tomasz Nurkiewicz.</p>

<p>To test the demo, simply clone the repository and edit the <a name="osvhost"><code>src/js/osvhost.js</code></a> to match the IP address and port for your OSv instance. (Don&rsquo;t forget to make sure that your OSv image includes the HTTP server).</p>

<p>Since I am runnning OSv compiled from source, I simply go to my OSv source tree and type:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="p-Indicator">&gt;</span><span class="err"> make image=httpserver,mgmt</span>
</span></code></pre></td></tr></table></div></figure>


<p><em>&hellip;&lt;chug, chug, chug></em>
And when it is done:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="p-Indicator">&gt;</span><span class="err"> ./scripts/run.py --api</span>
</span></code></pre></td></tr></table></div></figure>


<p>This will build and start an almost empty image, containing only the HTTP server, <a href="https://github.com/cloudius-systems/osv/wiki/Cloud-init">cloud init support</a> and the Java-based shell (not a very interesting appliance, I admit, but you can pick any image you prefer). Running like this, the REST API is available from <code>http://localhost:8000</code>, so this is what I enter into <a href="#osvhost"><code>src/js/osvhost.js</code></a>.</p>

<p>Then load the <code>jolokia-demo/src/index.html</code> in your favourite browser, and you should be greeted by this:</p>

<p><img src="http://osv.io/blog/images/jolokia-demo.png" alt="screenshot" /></p>

<p>As you can see, the demo provides the start of a small management console for Java with just a few lines of Javascript code, most of which actually deal with setting up the charts. Requesting and polling the actual data on the other hand is almost ridiculously easy.</p>

<p>Having Jolokia integrated in the OSv manageability layer means that not only can you access all the JMX attributes parallel with the rest of the exposed OSv aspects, not having to modify Java appliances, but also that you only need to deal with <a href="https://github.com/cloudius-systems/osv/wiki/The-RESTful-API#configuring-ssl">securing a single service point</a>.</p>

<p>This is just one small aspect of all the new and exciting manageability features that are in or coming to OSv. Over the next few months we hope to bring you additional aspects that will further enhance your deployment experience. Stay tuned.</p>

<p>If you have any questions on OSv management, please join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a>.  You can get general updates by subscribing to this blog&rsquo;s <a href="http://osv.io/blog/atom.xml">feed</a>, or folllowing <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OSv in the Spotlight at LinuxCon/CloudOpen 2014]]></title>
    <link href="http://osv.io/blog/blog/2014/08/20/linuxcon-cloudopen/"/>
    <updated>2014-08-20T16:10:31-07:00</updated>
    <id>http://osv.io/blog/blog/2014/08/20/linuxcon-cloudopen</id>
    <content type="html"><![CDATA[<p>While we relax and don&rsquo;t have to fix anything on our slides (really, they&rsquo;re all done), other speakers at the Linux Foundation&rsquo;s <a href="http://events.linuxfoundation.org/events/cloudopen-north-america">CloudOpen North America conference</a> in beautiful Chicago have some observations to make about OSv.</p>

<p><img src="http://osv.io/blog/images/chicago-river.jpeg" alt="Chicago River" /></p>

<p><strong>The Linux Foundation always picks great conference locations.</strong></p>

<p><a href="http://lccona14.sched.org/speaker/rcpavlicek">Russell Pavlicek</a> from the Xen project mentioned several library OSs that run on Xen in his <a href="http://lccona14.sched.org/event/17fdf31e5913cc4ebd5cf1f2ec039aa0">talk on new Xen features</a>.  He called the concept &ldquo;one of the biggest advances in the cloud.&rdquo;  Earlier library OSs have shown how much performance and simplicity gains are available, and OSv is extending the idea to ordinary POSIX and Java applications.</p>

<p><a href="http://lccona14.sched.org/speaker/mikeday">Mike Day</a> from IBM said, &ldquo;the engineers who write OSv are really good C++ coders,&rdquo; and called the project &ldquo;some of the finest C++ source code I&rsquo;ve ever seen&rdquo; in his <a href="http://lccona14.sched.org/event/434032efc316cc7aa98d4d590abda72e">talk on cloud operating systems for servers</a>.  He also had some praise for the <a href="http://osv.io/blog/blog/2014/04/19/spinlock-free/">spinlock-free way</a> that OSv handles mutexes, which as regular readers of this blog will know is important to prevent the dreaded lock-holder preemption problem.</p>

<p>If you&rsquo;re at LinuxCon, excuse me, #linuxcon, please come over and say &ldquo;hi&rdquo; to the OSv speakers: Don Marti and Glauber Costa. Hope to see you at the event, and please come to &ldquo;<a href="http://lccona14.sched.org/event/4684a80dd37f200277e971133920a2d0">Beating the Virtualization Tax for NoSQL Workloads With OSv</a>&rdquo; on Friday at 10:45 in the Colorado room.   Otherwise, you can get general OSv updates by subscribing to this blog&rsquo;s <a href="http://osv.io/blog/atom.xml">feed</a>, or folllowing <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>

<p><strong>Chicago River photo: <a href="http://commons.wikimedia.org/wiki/File:Chicago_river_2004.jpg">Urban for Wikimedia Commons</a></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Redis on OSv]]></title>
    <link href="http://osv.io/blog/blog/2014/08/14/redis-memonly/"/>
    <updated>2014-08-14T09:26:31-07:00</updated>
    <id>http://osv.io/blog/blog/2014/08/14/redis-memonly</id>
    <content type="html"><![CDATA[<p><strong>By Glauber Costa and Don Marti</strong></p>

<p>We&rsquo;re planning to attend the
Linux Foundation&rsquo;s <a href="http://events.linuxfoundation.org/events/cloudopen-north-america">CloudOpen North America conference</a>.  Hope to see you there, and please come to our talk, &ldquo;<a href="http://lccona14.sched.org/event/4684a80dd37f200277e971133920a2d0">Beating the Virtualization Tax for NoSQL Workloads With OSv</a>.&rdquo;</p>

<p>We&rsquo;ll be using a popular NoSQL database for our demo: Redis.  If you&rsquo;d like to follow along, you&rsquo;re welcome to clone and build Redis on OSv.  We&rsquo;re big Redis fans, because it&rsquo;s a fast, easy-to-administer, in-memory database that works with many useful data structures.</p>

<h2>Redis A to Z</h2>

<p><a href="http://redis.io/">Redis</a> is a remarkably useful piece of software.  People on the Internet talk about Redis a lot.  Here&rsquo;s what Google Suggest has to say about it: <strong>Atomic, benchmark, cluster, delete, expire, failover, gem, hash, incr, Java, key, list, master/slave, node.js, objects, Python, queue, Ruby, set, ttl, Ubuntu, &ldquo;vs. mongodb&rdquo;, Windows, XML, yum, zadd</strong>.  (<a href="http://redis.io/commands/ZADD">zadd</a> is a really cool command by the way.  A huge time-saver for maintaining &ldquo;scoreboard&rdquo; state for games and content-scoring applications.  Did we mention that we&rsquo;re Redis fans?)</p>

<p>Redis fills a valuable niche between memcached and a full-scale NoSQL database such as Cassandra.  Although it&rsquo;s fast and perfectly usable as a simple key-value store, you can also use Redis to manage more featureful data structures such as sets and queues.</p>

<p>It makes a great session cache, lightweight task queue, or a place to keep pre-rendered content or ephemeral data, and it&rsquo;s a <a href="http://highscalability.com/display/Search?moduleId=4876569&amp;searchQuery=redis">star at highscalability.com</a>.</p>

<p>But you probably already know that.</p>

<h2>Building Redis on OSv</h2>

<p>Redis works normally on OSv except for one feature: the
<a href="http://redis.io/commands/bgsave">BGSAVE</a>
command.  A Redis background
save depends on the operating system&rsquo;s
<a href="http://en.wikipedia.org/wiki/Copy-on-write#Copy-on-write_in_virtual_memory_management">copy-on-write</a>
functionity.  When you issue the BGSAVE
command, the parent Redis process calls
<a href="http://en.wikipedia.org/wiki/Fork_%28system_call%29">fork</a>,
and the parent process keeps running while the child
process saves the database state.</p>

<p>Copy-on-write ensures that the child process sees
a consistent set of data, while the parent gets its
own copy of any page that it modifies.</p>

<p>Because OSv has a single address space,
that isn&rsquo;t an option here. OSv support Redis
<a href="http://redis.io/commands/save">SAVE</a> but not BGSAVE. Other than that,
running redis on OSv requires little effort. From the OSv source tree,
all one should do is:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>make image=redis-memonly</span></code></pre></td></tr></table></div></figure>


<p>Now you have a <code>usr.img</code> file, which you can run locally with OSv&rsquo;s <code>run.py</code>.  Behind the scenes, all that this build step is doing is to issue the application&rsquo;s <code>make</code>, with the right set of flags so redis is a shared library.  For more info on how to do that, see <a href="http://osv.io/blog/blog/2014/04/03/capstan/">our earlier example</a>.  of how to use the <code>-fPIC</code> and <code>-shared</code> options.</p>

<h2>Is it fast?</h2>

<p>We have been running redis on local machines to test many of its
functionalities and help us mature OSv.  As with any piece of software, the
result of course depends on many factors.  Because OSv is an operating system
designed for the cloud, we wanted to showcase its performance running on Amazon
EC2.</p>

<p>To do that, we have selected the <a href="http://aws.amazon.com/ec2/instance-types/">c3.x8large</a> machines.  They feature 32 CPUs and
60Gb of memory each. We are fully aware this is an overkill in the case
of Redis &ndash; a single threaded application. However, those are the only machines
that Amazon advertises as featuring 10Gb networking, and we didn&rsquo;t want the
network to be a bottleneck for the sake of the benchmark. Also, smaller
machines cannot be put in EC2 placement groups. It all boils down to the network!</p>

<p>So in this benchmark, no more than two cores should be active at any given time &ndash; one for redis, one for network interrupt processing. In a real scenario, one could easily deploy in a smaller machine.</p>

<h3>Benchmark setup</h3>

<p>We have benchmarked redis&#8217; latest beta (beta-8) running both on OSv, and on an
Ubuntu14 AMI. To do that, we have just launched a new AMI, selected
Ubuntu14.04, and launched it. Once it launched, we have downloaded and compiled
redis&#8217; latest, and moved the redis.conf used by OSv to the machine. The only
difference in that configuration file from what is shipped with redis by
default, is that we disable disk activity. As already explained,
OSv currently do not support that, and to be fair, the Linux guest we are
comparing against should not hit the disk either at any point.</p>

<p>On ubuntu, redis was run with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>numactl --physcpubind=1 redis-server ~/redis.conf</span></code></pre></td></tr></table></div></figure>


<p>Using numactl considerably reduces the standard deviation coming from the Linux
scheduler moving the thread around.</p>

<p>The <code>redis-benchmark</code> command was issued in another machine of the same type,
running in the same zone and placement group.</p>

<p>The two commands were:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>numactl --physcpubind=1 redis-benchmark --csv -h &lt;IP&gt; -c 50 -n 100000 -P 1</span></code></pre></td></tr></table></div></figure>


<p>and later on, to demonstrate how OSv can handle larger messages,</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>numactl --physcpubind=1 redis-benchmark --csv -h &lt;IP&gt; -c 50 -n 100000 -P 16</span></code></pre></td></tr></table></div></figure>


<p>What this last command does, is to exercise redis&#8217; <code>pipeline</code> feature, that
can send multiple &ndash; in this case 16 &ndash; commands in the same packet. This will
decrease the impact of the round trip time in the final figure.</p>

<p>The difference can be clearly seen in the graph&hellip;</p>

<p><a href="http://osv.io/blog/images/redis.png"><img src="http://osv.io/blog/images/redis.png" alt="Redis benchmark results" /></a></p>

<p>Note that the LRANGE class of commands has a significantly different pattern
than the other commands. In that command, the client sends a very short query,
and receive a potentially very large reply, thereby exercising the transmission
path, rather than the receive path of OSv. This table shows that our transmission
path is lacking a bit of love, particularly when the response sizes grows (as the
pipeline level increases)</p>

<h2>Conclusions</h2>

<p>OSv is a fast maturing, but not yet mature operating system, soon to be in beta
phase. We have gaps to close, as can be seen in the case of LRANGE set of
benchmarks. So far, we have focused our efforts in technologies around the
receive path, and it has paid off: We can offer a level of performance far
beyond what an out of the box distribution can. Some features that we
architecturally lack, makes the use of Redis as a full-blown on-disk database
challenging. But if you want to serve your load from memory, the OSv promise
delivers: With OSv, you don&rsquo;t have to pay the virtualization tax.</p>

<p>If you&rsquo;ll be at CloudOpen, you can <a href="http://lccona14.sched.org/event/4684a80dd37f200277e971133920a2d0">add our talk to your schedule now</a>.</p>

<p>If you have any questions on running Redis or any other application, please join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a>.  You can get general updates by subscribing to this blog&rsquo;s <a href="http://osv.io/blog/atom.xml">feed</a>, or folllowing <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Clojure on OSv: Easier With a New Capstan Template]]></title>
    <link href="http://osv.io/blog/blog/2014/07/27/capstan-lein-template/"/>
    <updated>2014-07-27T00:00:00-07:00</updated>
    <id>http://osv.io/blog/blog/2014/07/27/capstan-lein-template</id>
    <content type="html"><![CDATA[<p>Clojure developers usually do not care too much about the underlying OS.
The low-level differences between Linux, Mac OS, and even Microsoft Windows are abstracted away by the JVM.</p>

<p>When deploying Clojure code on the cloud, there used to be one default choice &ndash; Linux.
But Linux
<a href="http://osv.io/blog/blog/2014/07/21/generic-os-is-dead/">is not an ideal OS</a>
for pure cloud services.</p>

<p><a href="https://github.com/cloudius-systems/osv">OSv</a> is a new, open source OS, designed specifically for the cloud.  Since OSv supports the standard JVM, it is ideal for running Clojure applications on the cloud.  And the same configuration applies to building VMs for any cloud: public clouds such as Amazon&rsquo;s and Google&rsquo;s, private clouds based on VMware or KVM, or public and private OpenStack.</p>

<p>Porting a Clojure application to OSv was already
<a href="http://osv.io/blog/blog/2014/04/22/riemann-on-osv/">pretty easy</a>, but
now it&rsquo;s even easier.  This blog post describes a new <a href="https://github.com/tzach/capstan-lein-plugin">lein template</a> for OSv.</p>

<!-- more -->


<h2>Usage</h2>

<p>Capstan works together with the <a href="http://leiningen.org/">Leinigen</a> build tool.</p>

<p>First, create a new project skeleton.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>lein new capstan new-app</span></code></pre></td></tr></table></div></figure>


<p>Now, you can run <a href="https://github.com/cloudius-systems/capstan">Capstan</a> to
run your project on an a OSv VM.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd new-app
</span><span class='line'>capstan run</span></code></pre></td></tr></table></div></figure>


<p>The template takes care of creating the project skeleton, including the Capstanfile.  When this is done, you can use Capstan directly to build a new OSv VM, deploy it on the cloud, or upload it to the public repository.</p>

<script type="text/javascript" src="https://asciinema.org/a/11068.js"
id="asciicast-11068" async="" data-speed="2" data-autoplay="1"
ata-size="medium"></script>


<p></p></p>

<p>For more info on Capstan and other OSv subjects, please join
the
<a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a>.<br/>
You can get updates on by subscribing to the <a href="http://osv.io/blog/atom.xml">OSv blog RSS feed</a> or following <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[If Java Application Servers Are Dead, So Is the Operating System (in the Cloud)]]></title>
    <link href="http://osv.io/blog/blog/2014/07/21/generic-os-is-dead/"/>
    <updated>2014-07-21T00:00:00-07:00</updated>
    <id>http://osv.io/blog/blog/2014/07/21/generic-os-is-dead</id>
    <content type="html"><![CDATA[<p><strong>By Tzach Livyatan</strong>
This post is a response to the excellent presentation <a href="http://www.slideshare.net/ewolff/java-application-servers-are-dead"><strong>&ldquo;Java Application Servers Are Dead!&rdquo;</strong></a> by <a href="https://twitter.com/ewolff">Eberhard Wolff</a>.
Go read his slides and come back here.</p>

<p>Back already?
Assuming you agree with Eberhard’s  claims,  let me demonstrate how
most of his points on Java Application Servers can be applied to a
generic OS (one designed for hardware servers) in the cloud as well.</p>

<!-- more -->


<p>First let me scope the discussion.
An operating system can run on your mobile, desktop, back office, or as a VM on the cloud.
For this post, I’m referring specifically to the cloud use case, which can be public or private.
Cloud deployments are the important case to concentrate on, as new Java application servers are mostly deployed on VMs these days.</p>

<p><img src="http://osv.io/blog/images/turtles.png" alt="turtles all the way down" />
Illustration: Omarcito for Wikimedia Commons (<a href="http://commons.wikimedia.org/wiki/File:Omarcito.gif">http://commons.wikimedia.org/wiki/File:Omarcito.gif</a>)</p>

<p>Eberhard present different properties of the Java Application Server, and for each, demonstrates why it is no longer relevant.
I will follow his footsteps, applying the same methodology to generic
OS, for two of the properties: <em>Container for multiple applications</em> and <em>Deployment</em>.</p>

<h2>Container for multiple applications</h2>

<p>Both the Java application server and the OS are supposed to isolate applications from each other. Both do a good job at it, and OS isolation is definitely somewhat stronger.  However, it is still not good enough for multitenancy, <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-2/">even with OS-level containers</a>.
This is why we have hypervisors, and this is why most deployment in the clouds include one application per VM.</p>

<h2>Deployment</h2>

<p>I agree with Eberhard’s claim that Java deployments (JAR, WAR, EAR) are problematic.
Linux-style packaging, using RPM or deb packages, is not a full solution either.</p>

<p>In a cloud environment, there is no reason to start with a blank OS, and spend 15 minutes downloading and installing the application. It makes more sense to use a pre-installed server image (AMI, in AWS terms) with the application already installed. Indeed, using a ready-made AMI is a common practice.</p>

<p>Containers are another successful attempt to fix this problem, but running containers on a virtual machine brings an extra layer of complexity. More on that <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-1/">here</a>. Obviously, there is still a requirement to install urgent patches on both AMI and containers.</p>

<p>To summarize, both the  Java application server and the generic OS were created to provide a set of services which is no longer required.</p>

<h2>With Java AS and generic OS dead, what&rsquo;s next?</h2>

<p><strong> Micro Services </strong></p>

<p><em>&ldquo;an approach to developing a single application as a suite of small
 services, each running in its own process and communicating with
 lightweight mechanisms, often an HTTP resource API&rdquo;</em>
<a href="http://martinfowler.com/articles/microservices.html">Martin Fowler</a>
<img src="http://osv.io/blog/images/PreferFunctionalStaffOrganization.png" alt="&quot;From micro services" />
Illustration: Martin Fowler (<a href="http://martinfowler.com/articles/microservices.html">http://martinfowler.com/articles/microservices.html</a>)</p>

<p>Each micro service can can have its own end to end stack, from the OS up to the application.
As explained above, an ideal scenario would be to deploy the micro service logic directly on a hypervisor, cutting two middle layers: the application server and the generic OS.</p>

<p>At this point you might doubt my sanity.
Run my application on EC2 with no OS at all to support it? Not quite.</p>

<p>As you recall from the “AS are dead” presentation, the application server has become an application library, dedicated to supporting a single application. With the Library OS concept, the the same process can also be applied to the OS, making it a library of the application.</p>

<p>For every micro service, one can use a tool like
<a href="http://osv.io/capstan">Capstan</a> to cook a new VM, pre integrating the
application, JVM and the OS &ndash; to a ready to be deployed VM. Just take
it and deploy it on your favorite cloud provider.</p>

<p>Take Capstan for a <a href="http://osv.io/run-locally/">spin</a></p>

<p>For more info on Capstan and other OSv subjects, please join
the
<a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a>.<br/>
You can get updates on by subscribing to the <a href="http://osv.io/blog/atom.xml">OSv blog RSS feed</a> or following <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With the OSv Shrinker API]]></title>
    <link href="http://osv.io/blog/blog/2014/07/14/osv-shrinker-api/"/>
    <updated>2014-07-14T08:37:05-07:00</updated>
    <id>http://osv.io/blog/blog/2014/07/14/osv-shrinker-api</id>
    <content type="html"><![CDATA[<p><strong>By Don Marti</strong></p>

<p>If you&rsquo;re writing a program that keeps a cache in memory, you&rsquo;re probably expecting users to have to set the cache size, which means a configuration setting or command-line argument.  And every configuration setting or command-line argument is something that you have to document, or explain to users when they get it wrong.</p>

<p>Thankfully, there&rsquo;s an easier way.</p>

<!-- more -->


<p>With OSv, you can ask the OS to let your program know when memory is tight, so that you can manage the cache size on the fly.  Less time spent tweaking settings, more items kept in cache, what&rsquo;s not to like?  Just set up a shrinker callback and register it.  (There is a <a href="https://www.kernel.org/doc/Documentation/cgroups/memory.txt">mechanism for memory pressure notifications</a> on Linux, but it&rsquo;s somewhat complex. With OSv it&rsquo;s just one function to write.)</p>

<h2>Defining a shrinker function</h2>

<p>OSv notifies your program of a low memory situation by
calling a shrinker callback.</p>

<p>A shrinker function takes two arguments:</p>

<ul>
<li><p>target amount of memory to free (size_t)</p></li>
<li><p>A boolean &ldquo;hard&rdquo; argument.  This is false if the function is being called for preemptive freeing of memory, and true if the system is under severe pressure.</p></li>
</ul>


<p>In most invocations, the argument <code>hard</code> will be set by OSv to false. This indicates that the system is acting preemptively, and memory pressure is starting to build up. The application is free to defer the freeing of objects to a later stage. Having your shrinker callback called with <code>hard</code> set to true, however, means that OSv is under severe memory pressure, and may be unable to serve allocations. If not enough memory is freed, the system may be forced to abort.</p>

<h2>Registering</h2>

<p>Now that the shrinker function is defined, you need to register it.</p>

<p>To register a shrinker function involves calling <code>osv_register_shrinker</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>void *osv_register_shrinker(const char *name,
</span><span class='line'>                            size_t (*func)(size_t target, bool hard));</span></code></pre></td></tr></table></div></figure>


<p>For example, an application in C can just do:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>extern void *osv_register_shrinker(const char *name,
</span><span class='line'>                            size_t (*func)(size_t target, bool hard));
</span><span class='line'>
</span><span class='line'>int main () {
</span><span class='line'>  ...
</span><span class='line'>  osv_register_shrinker("Example Shrinker", shrinker_function);
</span><span class='line'>  ...
</span><span class='line'>  return 0;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>An extremely basic example of a program that uses the OSv shrinker API is <a href="https://github.com/dmarti/memory-hog">memory-hog</a>.  To try it, install <a href="https://github.com/cloudius-systems/capstan">Capstan</a>, clone the Git repository, and build an OSv image to run it.  Currently memory-hog requires at least 2GB of memory.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git clone git@github.com:dmarti/memory-hog.git
</span><span class='line'>Cloning into 'memory-hog'...
</span><span class='line'>remote: Reusing existing pack: 76, done.
</span><span class='line'>remote: Total 76 (delta 0), reused 0 (delta 0)
</span><span class='line'>Receiving objects: 100% (76/76), 12.84 KiB | 0 bytes/s, done.
</span><span class='line'>Resolving deltas: 100% (39/39), done.
</span><span class='line'>Checking connectivity... done.
</span><span class='line'>$ cd memory-hog
</span><span class='line'>$ capstan run -m 2G
</span><span class='line'>Building memory-hog...
</span><span class='line'>Created instance: memory-hog
</span><span class='line'>OSv v0.09
</span><span class='line'>eth0: 192.168.122.15
</span><span class='line'>I'm a memory hog!
</span><span class='line'>program:  Oink!
</span><span class='line'>program:  Oink!
</span><span class='line'>program:  Oink!
</span><span class='line'>
</span><span class='line'>...many "Oink!"s later...
</span><span class='line'>
</span><span class='line'>program:  Oink!
</span><span class='line'>shrinker: Soft pressure, all done.
</span><span class='line'>program:  Oink!
</span><span class='line'>shrinker: Soft pressure, all done.
</span><span class='line'>program:  Oink!
</span><span class='line'>shrinker: Soft pressure, all done.
</span><span class='line'>program:  Oink!
</span><span class='line'>shrinker: Soft pressure, all done.
</span><span class='line'>program:  Oink!
</span><span class='line'>shrinker: Soft pressure, all done.
</span><span class='line'>program:  Oink!
</span><span class='line'>shrinker: Soft pressure, all done.
</span><span class='line'>shrinker: processing request to free 166062080 bytes.
</span><span class='line'>shrinker: starting with 64 things.
</span><span class='line'>shrinker: finishing with 58 things.
</span><span class='line'>      192534576 bytes of memory were freed!
</span><span class='line'>program:  Oink!
</span><span class='line'>shrinker: Soft pressure, all done.</span></code></pre></td></tr></table></div></figure>


<h2>Concurrency</h2>

<p>Shrinker callbacks conceptually work like asynchronous signal handlers,
They handle specific events and may be called at any time, from your program&rsquo;s point of view.</p>

<p>Each cache in your program that the shrinker can release data from,
must have its own lock to prevent concurrency issues.</p>

<p>The memory-hog sample program uses a mutex to handle concurrency.  To handle soft memory pressure, where your shrinker is called with <code>hard</code> set to false, you can keep a global <code>free_memory_please</code> global variable, and set it in the shrinker callback.  Then check it in the main loop, and free memory if necessary.  However, hard memory pressure must be handled immediately, or the system may hang.</p>

<h2>Handling soft pressure</h2>

<p>If possible, you should have your shrinker try to release some memory when called with <code>hard</code> set to false.  This will reduce the number of times you have to handle shrinking, and improve performance by making more free memory available to OSv for system tasks.  The memory-hog example currently ignores soft pressure.</p>

<p>For more info on memory management and other OSv subjects, please join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a>.  You can get updates on by subscribing to the <a href="http://osv.io/blog/atom.xml">OSv blog RSS feed</a> or folllowing <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Inside the OSv Boot Process]]></title>
    <link href="http://osv.io/blog/blog/2014/07/11/osv-boot/"/>
    <updated>2014-07-11T08:37:05-07:00</updated>
    <id>http://osv.io/blog/blog/2014/07/11/osv-boot</id>
    <content type="html"><![CDATA[<p>What happens in that critical fraction of a second between when a virtual x86_64 system &ldquo;powers up&rdquo; and when it begins running operating system code written in a high-level language?</p>

<p>For a helpful walk-through, there&rsquo;s a new article on the OSv wiki, originally written by Elazar Leibovich.</p>

<p><img src="http://osv.io/blog/images/pc-power.jpeg" alt="IBM PC power switch" /></p>

<p><strong>The boot process for a modern VM traces its history back to the
original IBM PC.</strong></p>

<p>While some of the intricate startup steps are historic, the end result is an OSv boot time that&rsquo;s less than a second&mdash;an order of magnitude faster than a conventional multi-user OS.  To read (or contribute!) details,
<a href="http://github.com/cloudius-systems/osv/wiki/OSv-early-boot-(MBR)">the &ldquo;OSv early boot&rdquo; article on the wiki.</a></p>

<p>(photo: <a href="http://commons.wikimedia.org/wiki/File:XT-PC-Power-Supply-PCB-IMG_0436.JPG">Hans Haase for Wikimedia Commons</a>. Available under the Creative Commons Attribution-Share Alike 3.0 Unported license.)</p>

<p>If you have any questions on OSv internals, or porting your application, please join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a>.  You can get general updates by subscribing to this blog&rsquo;s feed, or folllowing <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What Is the Most Popular OSv Virtual Appliance?]]></title>
    <link href="http://osv.io/blog/blog/2014/07/01/s3stat/"/>
    <updated>2014-07-01T00:00:00-07:00</updated>
    <id>http://osv.io/blog/blog/2014/07/01/s3stat</id>
    <content type="html"><![CDATA[<p><strong>By Tzach Livyatan</strong></p>

<p>(Spoiler: It&rsquo;s Apache Tomcat.)</p>

<p><a href="https://github.com/cloudius-systems/capstan">Capstan</a> is a tool for rapidly building and running applications on OSv.
As with Docker, Capstan users can download and run images from a public repository.
We chose to implement our public Capstan repository using <a href="http://aws.amazon.com/s3/">Amazon S3</a>.</p>

<p>Amazon S3  gives us the flexibility and security we need, but by default it&rsquo;s missing a critical feature: download statistics.
This statistics are very interesting to us, to evaluate which of the Capstan virtual appliances are more popular.  Fortunately, there is an easy way to gather the stats we need.</p>

<p>After a short tools survey, we choose <a href="http://www.s3stat.com/">s3stat</a>.</p>

<p><a href="http://www.s3stat.com/">s3stat</a> is a cloud-based service which can follow an S3 bucket, and visualize download statistics, by file, country, browser day, or otherwise.
The price makes sense, and it is super easy to enable.</p>

<p><img src="http://osv.io/blog/images/s3stat_chart.png" alt="s3stat chart" /></p>

<p>So what are the results? (drum roll&hellip;.)</p>

<!-- more -->


<p><img src="http://osv.io/blog/images/s3stat_files.png" alt="s3stat files" /></p>

<p>Omitting Capstan download of capstan index.yaml files, which Capstan does for every repository search, the most popular images are the base images for OSv and OSv + Java.
That make sense because these two images will be used by anyone who wants to build a local OSv application, running a native or Java application.</p>

<p>Virtual appliances comes right after, with (drums again&hellip;.) <strong>Tomcat</strong>, <strong>Cassandra</strong>, <strong>Memcached</strong> on the podium (Tomcat wins the Gold).
These are all very early results, but we will keep using s3stat to follow Capstan image downloads.</p>

<p><img src="http://osv.io/blog/images/s3stat_map.png" alt="s3stat map" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hypervisors Are Dead, Long Live the Hypervisor (Part 3)]]></title>
    <link href="http://osv.io/blog/blog/2014/06/23/containers-hypervisors-part-3/"/>
    <updated>2014-06-23T13:00:00-07:00</updated>
    <id>http://osv.io/blog/blog/2014/06/23/containers-hypervisors-part-3</id>
    <content type="html"><![CDATA[<p><strong>By Dor Laor and Avi Kivity</strong></p>

<h1>The new school: functionality, isolation and simplicity</h1>

<p>(This is part 3 of a 3-part series. <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-1/">Part 1</a>, <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-2/">Part 2</a>)</p>

<p>Containers make administration simple, and VMs give you portability, isolation, and administration advantages. The concept of putting <a href="http://www.slideshare.net/jpetazzo/linux-containers-lxc-docker-and-security">containers inside VMs</a> gives you the isolation you need, but there are now two layers of configuration and overhead instead of one.</p>

<p>What if there were one technology that could give us the simplicity and reduced overhead of containers and the security, tools, and hardware support of hypervisors? That’s where OSv comes in.  OSv, the “Operating System Designed for the Cloud.” takes an approach different from either containerization or virtualizing an existing bare-metal OS.  OSv is a single address space OS, designed to run as a guest only, with one application per VM.</p>

<h2>Best of both worlds?</h2>

<p>Glauber Costa, in a speech at Linuxcon called <a href="https://events.linuxfoundation.org/images/stories/pdf/lceu2012_costa.odp">&ldquo;The failure of Operating Systems, and how we can fix it&rdquo;</a>, pointed out that the existence of hypervisors is evidence that Operating Systems alone cannot meet some of the demands of real workloads. Through OSv, we have the opportunity to work together with the hypervisor to create a superior solution to what can be done with the OS alone: combining the resource efficiency of containers with the processor-aided advantages of hardware virtualization.</p>

<p>In the eight years since the release of Intel VMX, the silicon has kept getting better and better at moving the costs of virtualization into hardware. Enterprise customers have been demanding lower virtualization overhead for as long as hypervisors have been a thing, and the best minds of the CPU industry are working on it. With nested page tables and other features coming “for free” on the processor, virtualization overhead is being squeezed closer and closer to parity with bare metal.</p>

<p><img src="http://osv.io/blog/images/duplication.png" alt="typical cloud stack with duplication" /></p>

<p><strong>Typical cloud stacks have duplicate functionality at the hypervisor, guest OS, and application levels.</strong></p>

<p>While many players are trying to carve out a simple OS containerization system at the guest OS level, they are ignoring the stable, simple, secure, hardware-supported interface we already have: the hypervisor-guest interface.  There’s nothing that says we have to use this well-tested, industry-standard interface just to run a large, complete OS designed for bare metal. (In fact, research projects such as “Erlang on Xen” and MirageOS have explored using the hypervisor to run something less than a full OS for quite a while.)</p>

<h2>OSv is designed to perform</h2>

<p>OSv transparently loads an application into its kernel space. There is no userspace whatsoever. It removes the need for user to kernel context switches. In addition, the kernel trusts the application, since it relies on the underlying hypervisor for isolation from other applications in other VMs. Thus it opens up a way for the application to use any kernel API – from taking scheduling decisions to zero copy operations on data, and even unlock the brute force of the hardware page tables for the benefit of the application or its framework.</p>

<p>To date (June 2014), OSv provides 4x better performance for Memcache, a 40% gain with Apache Tomcat, and a 20% gain with Cassandra and SPECjbb. These results are based on our alpha versions, and are likely to improve as we complete the optimizations remaining on our roadmap.</p>

<p><img src="http://osv.io/blog/images/workloads.png" alt="OSv example workloads" /></p>

<p><strong>OSv runs many key cloud workloads with low overhead and high performance.</strong></p>

<p>OSv&rsquo;s image is your app and our kernel. Sometimes it means an image size of 10MB! That&rsquo;s a 100-400x better than the traditional OS and resembles a container&rsquo;s footprint. The OSv boot time is under a second, which is also closer to container startup time.</p>

<h2>OSv management: some questions for devops</h2>

<p>How many configuration files does your OS have? <strong>OSv has zero.</strong></p>

<p>How many times have you had to perform string manipulation on UNIX-like config files? <strong>OSv is built for automation and uses a RESTful API instead.</strong></p>

<p>How hard is it to upgrade your OS, and how can you revert it? <strong>OS is stateless.</strong></p>

<p>With an hypervisor below, you get the features such as live migration, perfect SLA, superior security for free while you get to enjoy from OSv’s added value.</p>

<h2>Capstan – or what we have learned from Docker</h2>

<p>We do love Docker with regard to development. The neat public image repository and the dead-simple-single execution won our hearts. We wanted to have the same for VMs, so we created the <a href="http://osv.io/capstan/">Capstan</a> project. Capstan has a public image repository, and by executing &lsquo;capstan run cloudius/osv-cassandra&rsquo; a virtual machine image will be either downloaded to your laptop (Mac OS X, Microsoft Windows, or Linux) or be executed on your cloud of choice. Capstan also allows you to build images including an app and a base OSv image. It takes about three seconds. On Capstan&rsquo;s roadmap, we plan to support the Docker file format, run Java apps directly without a config file, and form a simple PaaS for developers to load their favorite app directly to a running VM.</p>

<h2>Pick a cloud, any cloud</h2>

<p>The business case for cloud computing has never been better for the customer. While Amazon continues to upgrade the available instances and offer faster VMs at lower prices, Google is coming on strong as well. Microsoft, HP, IBM, and others are all competing for cloud business.  The cloud VM is the new generic PC.  Because we can create standard VMs that will run on anyone’s cloud, or on a private or hybrid cloud, we can develop with the confidence that we’ll be able to deploy to whatever infrastructure makes business sense&mdash;or move, or split deployment.</p>

<p>Lastly, we like to point out we are not against containers. Container technology is awesome when used for the right scenario. As there are cases for public transportation versus private cars, the same applies to devops. Both containers and OSv excel, in different domains. Here is a simple flow chart that can guide you with your choices:</p>

<p><a href="http://osv.io/blog/images/flowchart.png"><img src="http://osv.io/blog/images/flowchart.png" alt="Guest OS selection flowchart" /></a></p>

<p>Using OSv on ubiquitous, secure, full-featured hypervisors is the way to keep performance up, costs down, and options open. We had to completely reinvent the guest OS to do it&mdash;but now that we have it, OSv is available to build on. Please join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a> for technical info, or follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems on Twitter</a> for the latest news.</p>

<p>( <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-1/">Part 1</a>, <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-2/">Part 2</a>
)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hypervisors Are Dead, Long Live the Hypervisor (Part 2)]]></title>
    <link href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-2/"/>
    <updated>2014-06-19T13:00:00-07:00</updated>
    <id>http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-2</id>
    <content type="html"><![CDATA[<p><strong>By Dor Laor and Avi Kivity</strong></p>

<h1>Linux containers</h1>

<p>(This is part 2 of a 3-part series. <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-1/">Part 1 was published yesterday.</a>)</p>

<p>Containers, which create isolated compartments at the operating system level instead of adding a separate hypervisor level, trace their history not to mainframe days, but to Unix systems.</p>

<p>FreeBSD introduced “jails” in 2000. There’s a good description of them in  <a href="http://phk.freebsd.dk/pubs/sane2000-jail.pdf">Jails: Confining the omnipotent root by Poul-Henning Kamp and Robert N. M. Watson</a>. Solaris got its Zones in 2005.  Both systems allowed for an isolated “root” user and root filesystem.</p>

<p>The containers we know today, <a href="https://linuxcontainers.org/">Linux Containers</a>, or LXC, are not a single monolithic system, but more of a concept, based on a combination of  several different isolation mechanisms built into Linux at the kernel level.  <a href="https://lwn.net/Articles/587545/">Linux Containers 1.0 was released earlier this year.</a>, but many of the underlying systems have been under development in Linux independently.  Containers are not an all-or-nothing design decision, and it’s possible for different systems to work with them in different ways. <a href="https://linuxcontainers.org/">LXC can use all of the following Linux features</a>:</p>

<ul>
<li><p>Kernel namespaces (ipc, uts, mount, pid, network and user)</p></li>
<li><p>AppArmor and SELinux profiles</p></li>
<li><p>Seccomp policies</p></li>
<li><p>Chroots (using pivot_root)</p></li>
<li><p>Kernel capabilities</p></li>
<li><p>Control groups (cgroups)</p></li>
</ul>


<p>Although the combination can be complex, there are tools that make containers simple to use. For several years userspace tools such as LXC, libvirt allowed users to manage containers. However, containers didn’t really get picked up by the masses until the creation of <a href="https://www.docker.io/">Docker</a>. Docker and <a href="http://man7.org/linux/man-pages/man1/systemd-nspawn.1.html">systemd-nspawn</a> can start containers with minimal configuration, or from the command line. The Docker developers deserve much credit for adding two powerful concepts above the underlying container complexity:
a. Public image repository &ndash; immediate search and download of containers pre-loaded with dependencies, and
b. Dead-simple execution &ndash; a one-liner command for running a container.</p>

<p><img src="http://osv.io/blog/images/docker.png" alt="Docker diagram" /></p>

<p><strong><a href="https://www.docker.io/the_whole_story/">Docker gives container users a simple build process and a public repository system.</a></strong></p>

<h2>Container advantages</h2>

<p>When deployed on a physical machine, containers can eliminate the need of running two operating systems, one on top of the other (as in traditional virtualization). It makes IO system calls almost native and the footprint is minimal. However, this comes with a cost as we will detail below. The rule of the thumb is that if you do not need multi-tenancy and you’re willing to do without a bunch of software defined features, containers on bare metal are perfect for you!</p>

<p>In production, <a href="https://speakerdeck.com/jbeda/containers-at-scale">Google uses containers extensively</a>, starting more than two billion per week. Each container includes an application, built together with its dependencies, and containerization helps the company manage diverse applications across many servers.</p>

<p>Containers are an excellent case for development and test. It becomes possible to test some fairly complex setups, such as a version control system with hooks, or an SMTP server with spam filters,  by running services in a container. Because a container can use namespaces to get a full set of port numbers, it’s easy to run multiple complex tests at a time. The systemd project even uses containers for testing their software, which manages an entire Linux system. Containers are highly useful for testing because of their fast startup time&mdash;you’re just invoking an isolated set of processes on an existing kernel, not booting an entire guest OS.</p>

<p>If you run multiple applications that depend on different versions of a dependency, then deploying each application within its own container can allow you to avoid dependency conflict problems. Containers in theory decouple the application from the operating system. We use the term &lsquo;in theory&rsquo; because lots of care and thought should be given to maintaining your stack. For example, will your container combo be supported by the host OS vendor? Is your container up-to-date and does it include fixes for bugs such as ‘heartbleed’? Is your host fully updated, and does its kernel API provide the capabilities your application requires?</p>

<p>We highly recommend the use of containers whenever your environment is homogeneous:</p>

<ul>
<li><p>No multitenancy</p></li>
<li><p>Application is always written with clustering in mind</p></li>
<li><p>Load balancing is achieved by killing running apps and re-spinning them elsewhere (as opposed to live migration)</p></li>
<li><p>No need to run different kernel versions</p></li>
<li><p>No underlying hypervisor (otherwise, you&rsquo;re just adding a layer)</p></li>
</ul>


<p>When the above apply, you will enjoy near bare-metal performance, a small footprint and fast boot time.</p>

<h2>Container disadvantages: Security</h2>

<p>It’s clear that a public cloud needs strong isolation separating tenant systems. All that an attacker needs is an email address and a credit card number to set up a hostile VM on the same hardware as yours. But strong isolation is also needed in private clouds behind the corporate firewall. Corporate IT won’t be keen to run <strong>sandbox42.interns.engr.example.com</strong> and <strong>payroll.example.com</strong> within the same security domain.</p>

<p>Hypervisors have a relatively simple security model. The interface between the guest and the hypervisor is well defined, based on real or virtual hardware specifications. Five decades of hypervisor development have helped to form a stable and mature interface. Large portions of the interface&rsquo;s security are enforced by the physical hardware.</p>

<p>Containers, on the other hand, are implemented purely in software. All containers and their host share the same kernel. Nearly the entire Linux kernel had to undergo changes in order to implement isolation for resources such as memory management, network stack, I/O, the scheduler, and user namespaces. The Linux community is investing a lot of effort to improve and expand container support. However, rapid development makes it harder to stabilize and harden the container interfaces.</p>

<h2>Container disadvantages: Software-defined data center</h2>

<p>Hypervisors are the basis for the new virtualized data center. They allow us to perfectly abstract the hardware and play nicely with networking and storage.
Today there isn&rsquo;t a switch or a storage system without VM integration or VM-specific features.</p>

<p>Can a virtualized data center be based on containers in place of hypervisors? At almost all companies, no.. There will always be security issues with mounting SAN devices and filesystems from containers in different security domains. Yes, containers are a good fit for plenty of tasks but are restricted when it comes to sensitive areas such as you data center building blocks such as the storage and the network.</p>

<p>No one operating system, even Linux, will run 100% of the applications in the data center. There will always be diversity at the data center, and the existence of different operating systems will force the enterprise to keep the abstraction at the VM level.</p>

<h2>Container disadvantages: Management</h2>

<p>The long history of hypervisors means that the industry has developed a huge collection of tools for real-world administration needs.</p>

<p><img src="http://osv.io/blog/images/esx2.jpg" alt="CPM monitoring in VMware" /></p>

<p><strong><a href="http://robertmorannet.blogspot.com/2010/08/vmware-vsphere-screenshots.html">Blogger Robert Moran shows a screenshot of CPU monitoring in VMware’s vSphere</a>.</strong></p>

<p>The underlying functionality for hypervisor management is also richer. All of the common hypervisors support “live migration” of guests from one host to another.</p>

<p>Hypervisors have become an essential tool in the community of practice around server administration. Corporate IT is in the process of virtualizing its diverse collection of servers, running modern and vintage Linux distributions, plus legacy operating systems, and hypervisor vendors including VMWare and Microsoft are enabling it.</p>

<h2>Container disadvantages: Complexity</h2>

<p>While containers take advantage of the power built into Linux, they share Linux’s complexity and diversity.  For example, each Linux distribution standardizes on a different kernel version, and some use AppArmor while others use SELinux. Because containers are implemented using multiple isolation features at the OS level, the “containerization” features can vary by kernel version and platform.</p>

<h2>The anatomy of a multi-tenant exploit</h2>

<p>Let&rsquo;s assume a cloud vendor, whether SaaS, IaaS, or PaaS, implements a service within a container. How would an attacker exploit it?
The first stage would be to gain control of the application within the container. Many applications have flaws and the attacker would need to exploit an existing unpatched CVE in order to gain access to the container. IaaS even makes it simpler as the attacker already has a “root” shell inside a neighboring container.</p>

<p>The next stage would be to penetrate the kernel. Unfortunately, the kernel&rsquo;s attack surface contains hundreds of system calls, and other vulnerabilities exist in the form of packets and file metadata that can jeopardize the kernel. Many attackers have access to zero-day exploits, unpublished local kernel vulnerabilities. (A typical “workflow” is to watch upstream kernel development for security-sensitive fixes, and figure out how to exploit them on the older kernels in production use.)</p>

<p>Once the hacker gains control of the kernel, it&rsquo;s game over. All the other tenants’ data is exposed.</p>

<p>The list of exploitable bugs is always changing, and there will probably be more available by the time you read this.  A few recent examples:</p>

<ul>
<li><p>“An information leak was discovered in the Linux kernel&rsquo;s SIOCWANDEV ioctl call. A local user with the CAP_NET_ADMIN capability could exploit this flaw to obtain potentially sensitive information from kernel memory.“ (CVE-2014-1444) Some container configurations have CAP_NET_ADMIN, while others don’t. Because it’s possible to set up containers in more or less restricted ways, individual sites need to check if they’re vulnerable. (Many LInux capabilities are <a href="http://forums.grsecurity.net/viewtopic.php?f=7&amp;t=2522">equivalent to root</a> because they can be used to obtain root access.)</p></li>
<li><p>“An information leak was discovered in the wanxl ioctl function in  Linux. A local user could exploit this flaw to obtain potentially sensitive information from kernel memory.” (CVE-2014-1445)”</p></li>
<li><p>“An unprivileged local user with access to a CIFS share could use this flaw to crash the system or leak kernel memory. Privilege escalation cannot be ruled out (since memory corruption is involved), but is unlikely.“ (CVE-2014-0069)</p></li>
</ul>


<p>Each individual vulnerability is usually fixed quickly, but there’s a constant flow of new ones for attackers to use. <a href="https://lwn.net/Articles/462756/">Linux filesystem developer Ted Ts’o wrote</a>,</p>

<blockquote><p>Something which is baked in my world view of containers (which I suspect is not shared by other people who are interested in using containers) is that given that the kernel is shared, trying to use containers to provide better security isolation between mutually suspicious users is hopeless.  That is, it&rsquo;s pretty much impossible to prevent a user from finding one or more zero day local privilege escalation bugs that will allow a user to break root.  And at that point, they will be able to penetrate the kernel, and from there, break security of other processes.</p>

<p>So if you want that kind of security isolation, you shouldn&rsquo;t be using containers in the first place.  You should be using KVM or Xen, and then only after spending a huge amount of effort fuzz testing the KVM/Xen paravirtualization interfaces.</p></blockquote>

<p><a href="http://permalink.gmane.org/gmane.linux.coreos.devel/287">Kernel developer Greg Kroah-Hartman wrote</a>,</p>

<blockquote><p>Containers are not necessarily a &ldquo;security&rdquo; boundary, there are many &ldquo;leaks&rdquo; across it, and you should use it only as a way to logically partition off users/processes in a way that makes it easier to manage and maintain complex systems. The container model is quite powerful and tools like docker and systemd-nspawn provide a way to run multiple &ldquo;images&rdquo; at once in a very nice way.</p></blockquote>

<p>Containers are powerful tools for Linux administrators, but for true multi-tenant cloud installations, we need stricter isolation between tenants.</p>

<p>Containerization is not “free”. For instance, the Linux Memory Controller can slow down the kernel by as much as 15%, just by being enabled, with no users. The Memory Controller itself is complicated, but cgroups controllers, on which it depends, are also complex. The surface of change is just way too big, and the resulting implementation necessarily too complex. <a href="https://plus.google.com/109487070944143253361/posts/6AdwyTfPFQe">George Dunlap said it best</a>,</p>

<blockquote><p>With containers you&rsquo;re starting with everything open and then going around trying to close all the holes; if you miss even a single one, bam, you lose. With VMs, you start with almost everything closed, and selectively open things up; that makes a big difference.</p></blockquote>

<p><strong>This is part 2 of a 3-part series.</strong> Please subscribe to our <a href="http://osv.io/blog/atom.xml">feed</a> or follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> to get a notification when part 3 is available.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hypervisors Are Dead, Long Live the Hypervisor (Part 1)]]></title>
    <link href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-1/"/>
    <updated>2014-06-19T13:00:00-07:00</updated>
    <id>http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-1</id>
    <content type="html"><![CDATA[<p><strong>By Dor Laor and Avi Kivity</strong></p>

<p>The hypervisor is the basic building block of cloud computing; hypervisors drive the software-defined data center revolution, and two-thirds of all new servers are virtualized today. Hypervisors for commodity hardware have been the key enabler for the software revolution we have been experiencing.</p>

<p>However, for the past 8 years a parallel technology has been growing, namely, containers. Recently containers have been getting a fairly large amount of traction with the development of the Docker project. When run on bare metal, containers perform better than hypervisors and have a lower footprint.</p>

<p>There is a lot in common with the goals of these technologies. These three blog entries will try to provide an answer to the question</p>

<p><strong>Will containers kill the hypervisor?</strong></p>

<p>The series will provide in-depth explanations about the underlying technology and the pros and cons of each solution.</p>

<h2>Intro: ancient hypervisor lore</h2>

<p>What is virtualization anyway?</p>

<p><img src="http://osv.io/blog/images/virtualization.png" alt="Virtualization diagram" /></p>

<p>A hypervisor is a software component (potentially assisted by hardware) that allows us to run multiple operating systems on the same physical machine. The overlay OS is called the guest OS or simply, a Virtual Machine (VM). The guest OS may not even be aware it is running on virtual hardware.</p>

<p>The interface between the guest and the host is the hardware specification. It covers the CPU itself and any other hardware devices, from BIOS to NICs, SCSI adapters, GPUs and memory.</p>

<p><img src="http://osv.io/blog/images/IBM360-67AtUmichWithMikeAlexander.jpg" alt="IBM System/360" /></p>

<p><strong>IBM, together with MIT and the University of Michigan, pioneered hypervisor technology on System/360 and System/370 mainframes, beginning in the 1960s.</strong></p>

<p>IBM was the first company to produce hypervisors. The IBM System/360, model 1967, was the first to <a href="http://www.beagle-ears.com/lars/engineer/comphist/ibm360.htm#gener">ship with virtual memory hardware supporting virtualization</a>. The next system in the series, System/370, was the “private cloud” of its day. Administrators could set up virtual machines for running different OS versions, and even public-cloud-like “time sharing” by multiple customers.</p>

<h2>Virtualization for x86</h2>

<p>Virtualization didn’t make it to commodity systems until the <a href="http://www.vmware.com/company/news/mediaresource/milestones">release of VMware Workstation in 1999</a>. In the early 2000s, hypervisors were based on pure software and were mostly useful for development and testing.  VMware initially used a technique called dynamic translation to intercept privileged operations by the guest operating system. When the guest accessed “hardware”, VMWare rewrote the instructions on the fly, to protect itself from the guest and isolate guests from each other.</p>

<p><img src="http://osv.io/blog/images/vmware-logo.png" alt="VMware logo" /></p>

<p>Later on, the open source Xen hypervisor project coined the term paravirtualization (PV). PV guests, which have to be specially modified to run on a PV host, do not execute privileged instructions themselves but ask the hypervisor to do it on their behalf.</p>

<p><img src="http://osv.io/blog/images/xen-logo.png" alt="Xen logo" /></p>

<p>Eventually, Intel, AMD and ARM implemented support for virtualization extensions. A special host mode allows running guest code on the bare CPU, getting near 100% of bare metal throughput for CPU-intensive workloads. In parallel, the memory management and the IO path received attention as well with technologies such as nested paging (virtual memory), virtual interrupt controllers, single-root I/O virtualization (SRIOV) and other optimizations.</p>

<h2>Hardware support for hypervisors</h2>

<p>Hypervisor enablement continues to be a priority for hardware manufacturers. <a href="https://plus.google.com/+OsvIo/posts/fgzsepcScTa">Glauber Costa wrote</a>, “the silicon keeps getting better and better at taking complexity away from software and hiding somewhere else.”</p>

<p><a href="http://www.redhat.com/rhecm/rest-rhecm/jcr/repository/collaboration/jcr:system/jcr:versionStorage/5e7884ed7f00000102c317385572f1b1/1/jcr:frozenNode/rh:pdfFile.pdf">According to a paper from Red Hat Software</a>,</p>

<blockquote><p>Both Intel and AMD continue to add new features to hardware to improve performance for virtualization. In so doing, they offload more features from the hypervisor into “the silicon” to provide improved performance and a more robust platform&hellip;.These features allow virtual machines to achieve the same I/O performance as bare metal systems.</p></blockquote>

<h2>Old-school hypervisors</h2>

<p>Hypervisors are one of the main pillars of the IT market (try making your way through downtown San Francisco during VMworld) and solve an important piece of the problem. Today the hypervisor layer is commoditized, users can choose any hypervisor they wish when they deploy Open Stack or similar solutions.</p>

<p>Hypervisors are a mature technology with a rich set of tools and features ranging from live migration, cpu hotplug, software defined networking and other new coined terms that describe the virtualization of the data center.</p>

<p>However, in order to virtualize your workload, one must deploy a full fledged guest operating system onto every VM instance. This new layer is a burden in terms of management and in terms of performance overhead. We’ll look at one of the other approaches to compartmentalization next time: containers.</p>

<p><strong>This is part 1 of a 3-part series.</strong>  <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-2/">Part 2 is now available</a>.  Please subscribe to our <a href="http://osv.io/blog/atom.xml">feed</a> or follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> to get a notification of future posts.</p>

<p>Photo credit, IBM 360: <a href="http://commons.wikimedia.org/wiki/File:IBM360-67AtUmichWithMikeAlexander.jpg">Dave Mills for Wikimedia Commons</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nadav Har’El Presenting OSv at USENIX June 19th]]></title>
    <link href="http://osv.io/blog/blog/2014/06/18/usenix-atc/"/>
    <updated>2014-06-18T08:54:23-07:00</updated>
    <id>http://osv.io/blog/blog/2014/06/18/usenix-atc</id>
    <content type="html"><![CDATA[<p><strong>By Don Marti</strong></p>

<p>If you&rsquo;re attending the USENIX Annual Technical Conference, be sure not to miss
<a href="https://www.usenix.org/conference/atc14/technical-sessions/presentation/kivity">&ldquo;OSv—Optimizing the Operating System for Virtual Machines&rdquo; </a>. Nadav Har&#8217;El will be presenting tomorrow at 10:40 AM.</p>

<p>Here&rsquo;s a quick preview of some of the performance results that Nadav will show:</p>

<p><img src="http://osv.io/blog/images/usenix-paper.png" alt="table from upcoming paper" /></p>

<p>It&rsquo;s also your opportunity to ask Nadav some hard questions.</p>

<p>If you&rsquo;re not at USENIX this year, you can still get a copy of the paper.</p>

<p>Thanks to the USENIX open access policy, the paper is scheduled to go Open Access on the day of the event. To get an alert from us when it&rsquo;s up, please follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems on Twitter</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Capstan With Local OSv Images]]></title>
    <link href="http://osv.io/blog/blog/2014/06/06/capstan-push/"/>
    <updated>2014-06-06T08:54:23-07:00</updated>
    <id>http://osv.io/blog/blog/2014/06/06/capstan-push</id>
    <content type="html"><![CDATA[<p><strong>By Don Marti</strong></p>

<p>If you&rsquo;re building OSv from source, you can use the <code>capstan push</code> command to temporarily use a local build in place of a base image from a network repository.  This is handy when you&rsquo;re trying your application with a patched version of OSv.   Just run <code>capstan push</code> after the OSv build to push your newly built image into your local Capstan repository.</p>

<p>For example, if your Capstanfile uses the <code>cloudius/osv-base</code> base image:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>make 
</span><span class='line'>capstan push cloudius/osv-base  build/release/usr.img</span></code></pre></td></tr></table></div></figure>


<p>When you&rsquo;re ready to go back to using the image from the network, you can run</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>capstan pull cloudius/osv-base</span></code></pre></td></tr></table></div></figure>


<p>to replace the image in your local repository with the image from the network repository.</p>

<p>For more tips and updates, please follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems on Twitter</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OSv Paper Coming to USENIX in June]]></title>
    <link href="http://osv.io/blog/blog/2014/05/19/usenix-atc/"/>
    <updated>2014-05-19T08:54:23-07:00</updated>
    <id>http://osv.io/blog/blog/2014/05/19/usenix-atc</id>
    <content type="html"><![CDATA[<p><strong>By Don Marti</strong></p>

<p>We&rsquo;re going to the USENIX Annual Technical Conference
in Philadelphia!</p>

<p><a href="https://www.usenix.org/conference/fcw14"><img src="https://www.usenix.org/sites/default/files/fcw14_banner_450x93.png" border="0" alt="2014 Federated Conferences Week"></a></p>

<p><a href="https://www.usenix.org/conference/atc14/technical-sessions/presentation/kivity">Our paper, &ldquo;OSv—Optimizing the Operating System for Virtual Machines&rdquo; </a> has been accepted by one of our favorite IT events.  We appreciate all the excellent comments and questions from our peer reviewers.</p>

<p>This year, ATC will be part of a Federated Conferences Week that includes HotCloud, HotStorage, two days of sysadmin training, and more, so there should be something for everyone.</p>

<p>The paper will be available under Open Access terms starting on the date of the event, but we all hope you can come see us live and in person.</p>

<p>Here&rsquo;s the abstract:</p>

<blockquote>
<p>Virtual machines in the cloud typically run existing general-purpose operating systems such as Linux. We notice that the cloud’s hypervisor already provides some features, such as isolation and hardware abstraction, which are duplicated by traditional operating systems, and that this duplication comes at a cost.</p>

<p>We present the design and implementation of OSv, a new guest operating system designed specifically for running a single application on a virtual machine in the cloud. It addresses the duplication issues by using a low-overhead library-OS-like design. It runs existing applications written for Linux, as well as new applications written for OSv . We demonstrate that OSv is able to efficiently run a variety of existing applications. We demonstrate its sub-second boot time, small OS image and how it makes more memory available to the application. For unmodified network-intensive applications, we demonstrate up to 25% increase in throughput and 47% decrease in latency. By using non-POSIX network APIs, we can further improve performance and demonstrate a 290% increase in Memcached throughput.</p>
</blockquote>


<p>For more event updates, please follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems on Twitter</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interview: OSv on 64-bit ARM Systems]]></title>
    <link href="http://osv.io/blog/blog/2014/05/12/osv-on-64-bit-arm/"/>
    <updated>2014-05-12T20:54:23-07:00</updated>
    <id>http://osv.io/blog/blog/2014/05/12/osv-on-64-bit-arm</id>
    <content type="html"><![CDATA[<h2>Q&amp;A with Paul Mundt,  Jani Kokkonen, and Claudio Fontana</h2>

<p>Paul Mundt is CTO of OS &amp; Virtualization at Huawei, while Jani and Claudio are both Virtualization Architects on Huawei&rsquo;s virtualization team. All are based in Munich, which is the headquarters for Huawei’s European Research Center. The company also has a team of OSv developers in Hangzhou, China, who are focused on adaptation of OSv to Huawei&rsquo;s x86-based enterprise servers.</p>

<p><strong>Q: ARM processors are everywhere.  What are the important differences
between the Aarch64 hardware that you&rsquo;re targeting with the OSv port and
the garden-variety ARM processors that we have in our phones, toasters,
and Raspberry Pis?</strong></p>

<p>Other than the relatively obvious architectural differences in going from a 32-bit to a 64-bit architecture (more general purpose registers, address space, etc), there are quite a number of fundamental changes in v8 that considerably clean up the architecture in contrast to earlier versions.</p>

<p>One of the more substantial changes is the new exception and privilege model, with 4 exception levels now taking the place of v7&rsquo;s assortment of processor modes. The new privilege levels are much more in line with conventional CPU privilege rings (eg, x86), even though for whatever reason the numbering has been inverted &mdash; now with EL3 being the most privileged, and EL0 being the least.</p>

<p>Of specific relevance to the OSv port, through its heavy use of C++11/C1x atomic operations and memory model, are the improvements to the CPU&rsquo;s own memory and concurrency model. In contrast to x86, v7 and earlier adopt a weak memory model for better energy efficiency, but have always been terrible at sequentially consistent (SC) atomics as a result. In v8, the weak memory model has been retained, but special attention has also been paid to improving the deficiencies in SC atomics, resulting in the addition of load-acquire/store-release instruction pairs that work across the entire spectrum of general purpose and exclusive loads/stores. This places the architecture in direct alignment with the emerging standardization occurring in C++11/C1x, and has simplified much of the porting work in this area.</p>

<p>Beyond this (while not strictly v8-specific) there are also a new range of virtualization extensions to the interrupt controller that we can take advantage of, but unfortunately this part of the IP is not yet finalized and remains under NDA.</p>

<p>As our semiconductor company (HiSilicon) produces its own Aarch64 CPUs, we have also made a number of our own changes to the microarchitecture to better fit our workloads, especially in the areas of the cache and virtual memory system architecture, virtualization extensions, interconnects, and so on.</p>

<p><strong>Q: What class of applications is your team planning to run on OSv?</strong></p>

<p>We see many different potential applications for OSv within Huawei. While OSv is primarily touted as a lightweight cloud OS, the area that is more interesting for my team is its potential as a lightweight kernel for running individual applications directly on the hypervisor, as well as its ability to be used as an I/O or compute node kernel in the dataplane through virtio.</p>

<p>Tight coupling of the JVM to the hypervisor is also an area that we are interested in, particularly as we look to new directions in heterogeneous computing emerging through OpenJDK Sumatra, Aparapi, and the on-going work by the HSA Foundation in which we are also engaged.</p>

<p>Over the next year or so we also expect to see the JVM support maturing, to the point where it should also become possible to run some of the heavier weight big data stacks, but there is a long way to go first.</p>

<p><strong>Q:  When you&rsquo;re considering using OSv as a lightweight kernel for running applications directly on the hypervisor, are you considering using it  without a local filesystem?  (I understand OSv can boot in about 1/10th the time without ZFS.)</strong></p>

<p>ZFS is indeed quite heavyweight for our needs, and indeed, up until this stage in the porting effort we have largely been able to avoid it, but this will obviously not be the long-term case as we look to a solution we can bring to our customers.</p>

<p>In addition to the boot time issues you have mentioned, the ZFS adaptive replacement cache (ARC) and page cache interactivity problems with large mmap()&rsquo;s is an area of concern for some of our potential users, so this is something that we are also closely monitoring, particularly as we think about other ways we might utilize OSv for other applications in the future.</p>

<p>That being said, at the moment we basically see a few different directions to go on the file system side (and by extension, the VFS layer) for our more immediate applications:</p>

<p>1) Simple in-memory file systems with substantially reduced functionality that we can use for scenarios like dataplane applications or I/O nodes where we need no persistent storage. In these cases as we don&rsquo;t even need to support file I/O, we will likely be carrying out some customization and optimizations in this area. This is obviously in contrast to the compute node and control plane side, which we primarily intend to run under Linux in parallel for now.</p>

<p>2) Adaptation for phase change and other non-volatile memories. OSv has a much lighter weight stack with no real legacy at the moment, so fits the role of testbed quite well in terms of experimenting with the best way to tie these technologies in for different deployment scenarios, particularly under a layer of virtualization. In the long run we would naturally expect the results of this work to transfer to the Linux kernel, too.</p>

<p>3) Global and distributed filesystems &mdash; initially across OSv instances, and then across to Linux. This also has implications for the underlying transport mechanisms, particularly as we look to things like lightweight paravirtualized RDMA and inter-VM communication.</p>

<p><strong>Q:  Which hypervisor or hypervisors are you using?</strong></p>

<p>While Huawei is actively engaged across many different hypervisors, as my department (in which most of us have a Linux kernel development background) is quite focused on working close to the metal and on performance related issues, KVM is our primary focus.</p>

<p>We have previously done a fair bit of work with guest OS real-time, inter-VM communications, and I/O virtualization enhancements on ARM, so continuing with KVM also makes the most sense for us and our customers.</p>

<p>As one of the main focuses for my OS team is in heterogeneous computing, we also aim to leverage and contribute to much of the work surrounding accelerator, domain processor, and heterogeneous system architecture virtualization under KVM, although much of this is tied up in various European Union framework programmes (eg, FP7-ICT, H2020) at the moment. OSv will also continue to play an important role in these areas as we move forward.</p>

<p><strong>Q: Anything else that you would like to add?</strong></p>

<p>Only that now is an exciting time to be in OSv development. The OS has a lot of potential and is still very much in its infancy, which also makes it an excellent target for trying out new technical directions. I would also encourage people who are not necessarily cloud-focused to look at the broader potential for the system, as there&rsquo;s certainly a lot of interesting development to get involved in.</p>

<h2>About</h2>

<p><img src="http://osv.io/blog/images/Paul_Mundt.jpg" alt="Paul Mundt" /></p>

<p><strong>Paul Mundt</strong> is the CTO of OS &amp; Virtualization at Huawei’s European Research Center in Munich, Germany, where he manages the Euler department (including OS &amp; Virtualization R&amp;D, as well as new CPU development and support). Paul first joined Huawei at the beginning of 2013 as the Chief Architect of Huawei’s Server OS division, responsible for overall architecture and strategy. Prior to that, as the Linux Kernel Architect at Renesas in Tokyo, Paul was responsible for leading the Linux group within Renesas for seven years, establishing both the initial strategy and vision while taking the group from zero in-house support or upstream engagement to supporting hundreds of different CPUs across the entire MCU/MPU spectrum and becoming a consistent top-10 contributor to the Linux kernel, which carries on to this day. He has more than 15 years of Linux kernel development experience, across a diverse range of domains (HPC, embedded, enterprise, carrier grade), and has spent most of that time as the maintainer of various subsystems (primarily in the areas of core kernel infrastructure, CPU architectures, memory management, and file systems). He previously organized and chaired the Memory Management Working Group within the CE Linux Forum, where he advocated the convergence of Enterprise and Embedded technologies, resulting in the creation of Asymmetric NUMA, as well as early transparent superpage/large TLB adoption. He is a voting member of the OASIS Virtual I/O Device (VIRTIO) Technical Committee and the HSA Foundation.</p>

<p><img src="http://osv.io/blog/images/Jani_Kokkonen.jpg" alt="Jani Kokkonen" /></p>

<p><strong>Jani Kokkonen</strong> received his master’s degree in 2000 from the Technical University of Helsinki, Finland. He went to pursue research and development job in Nokia Networks. The work concentrated in research of different transport technologies on various radio access networks. This was followed by research and development activities on virtualization technologies on 3GPP radio and core network elements. Work consisted also evaluation of hardware extensions for virtualization support on various embedded multicore chips. He has been as virtualization architect in Huawei ERC Euler Department since September 2011. The work in Huawei has concentrated on research and development on QEMU/KVM on ARM and Intel platforms varying from CPU to network technologies, with his most recent effort focusing on ARM64 memory management where he is responsible for the MMU backend in the Aarch64 OSv port, as well as leading the OSv team. He is a member of the OASIS Virtual I/O Device (VIRTIO) Technical Committee and the Multicore Association.</p>

<p><img src="http://osv.io/blog/images/Claudio_Fontana.jpg" alt="Claudio Fontana" /></p>

<p><strong>Claudio Fontana</strong> received his Laurea in Computer Science in 2005 at the University of Trento, Italy, discussing a thesis on frameworks for the evaluation of taxonomy matching algorithms. He went on to pursue a software engineering opportunity in Zurich, where he worked on medium-scale (100s hosts) distributed systems. This was followed by a software engineering position in Amsterdam, working on messaging, routing, firewalls and billing systems. He has been with Huawei since December 2011. He is currently working in the virtualization area (Linux/KVM/QEMU). He is part of the early enablement effort for the ARM 64bit architecture (ARMv8 AArch64), and has been a maintainer and contributor of Free and Open Source projects, lately involving mostly QEMU binary translation (as QEMU Aarch64 TCG maintainer) and the OSv Operating System (as Aarch64 maintainer). He also spent some time as a member of Linaro&rsquo;s Virtualization team, where he focused on early Aarch64 enablement.</p>

<h2>For more information</h2>

<p>To keep up with the progress of OSv on ARM (and x86_64 too), join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a> or follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bridged Networking With Capstan]]></title>
    <link href="http://osv.io/blog/blog/2014/05/07/capstan-bridge/"/>
    <updated>2014-05-07T08:37:05-07:00</updated>
    <id>http://osv.io/blog/blog/2014/05/07/capstan-bridge</id>
    <content type="html"><![CDATA[<p><strong>By Don Marti</strong></p>

<p>New versions of <a href="https://github.com/cloudius-systems/capstan">Capstan</a> are making it simpler to run OSv virtual machines in a production configuration, by adding more control of network options.  A useful new feature, which helps deal with the <a href="https://github.com/cloudius-systems/osv/wiki/Running-OSv-image-under-KVM-QEMU">details of bringing up networking</a>, is the <code>-n</code> option.</p>

<p>By default, Capstan starts up KVM/QEMU with user networking:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> -netdev user,id=un0,net=192.168.122.0/24,host=192.168.122.1</span></code></pre></td></tr></table></div></figure>


<p>(That&rsquo;s from <code>ps ax | grep qemu</code>, which you can run
to see the qemu-system-x86_64 command that Capstan
is executing for you.)</p>

<p>But there are many more <a href="http://www.linux-kvm.org/page/Networking">networking options</a> for QEMU/KVM.  The basic user networking, which does not require root access to start up, is good for development and simple tasks.  But for production use, where you need to get your VM on a network where it&rsquo;s available from other VMs or from the outside, you&rsquo;ll need bridged networking.  (See your Linux distribution or hypervisor documentation for the details of creating a virtual or public bridge device.)</p>

<p>If you invoke <code>capstan run</code> with the <code>-n bridge</code> option, you&rsquo;ll get QEMU running with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-netdev bridge,id=hn0,br=virbr0,helper=/usr/libexec/qemu-bridge-helper</span></code></pre></td></tr></table></div></figure>


<p>If you have a specific bridge device to connect to, you can use the <code>-b</code> option with the name of the bridge device.  The default is <code>virbr0</code>, but you can also set up a public bridge, usually <code>br0</code>, that&rsquo;s bridged to a physical network interface on the host.</p>

<h1>Other hypervisors</h1>

<p>Don&rsquo;t feel left out if you have a different hypervisor.  Capstan also handles bridged networking on VirtualBox, with the rest of the supported hypervisors coming soon.   The fact that the syntax is the same is going to be a big time-saver for those of us who have to do testing and demos on multiple systems&mdash;no more dealing with arcane commands that are different from system to system.</p>

<p>For more on Capstan and networking, please join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list on Google Groups</a>.  You can get updates by subscribing to this blog&rsquo;s feed, or folllowing <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New OSv Meetup Group]]></title>
    <link href="http://osv.io/blog/blog/2014/04/25/meetup/"/>
    <updated>2014-04-25T08:54:23-07:00</updated>
    <id>http://osv.io/blog/blog/2014/04/25/meetup</id>
    <content type="html"><![CDATA[<p><strong>By Don Marti</strong></p>

<p>We held the first meeting of the OSv <a href="http://www.meetup.com/OSv-Developer-Meetup/">Meetup group</a> in San Francisco this week, and got 14 participants from the Apache, Big Data, and OSv communities, as well as a few meetup.com users interested in cloud computing who just came along serendipitously.</p>

<p><img src="http://osv.io/blog/images/meetup.jpg" alt="attendees" /></p>

<p>Thanks to our hosts at <a href="http://ohmdata.com/">OhmData</a> who made their groovy South of Market office space available, and thanks to our attendees for coming in to try out OSv.  Looking forward to seeing the results of your initial experiments.</p>

<p>(For the users of VirtualBox on Mac OS who ran into the <a href="https://groups.google.com/forum/#!topic/osv-dev/yobHBsusLN8">&ldquo;assertion failed&rdquo; problem</a>, we&rsquo;re discussing that on the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a> now, so watch the list for an update.)</p>

<p>To get advance notice of future events&mdash;both the free-form hands-on sessions like this one and an upcoming tech talk series&mdash;please join the <a href="http://www.meetup.com/OSv-Developer-Meetup/">Meetup group</a> or follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Riemann - a Clojure Application on OSv]]></title>
    <link href="http://osv.io/blog/blog/2014/04/22/riemann-on-osv/"/>
    <updated>2014-04-22T09:00:00-07:00</updated>
    <id>http://osv.io/blog/blog/2014/04/22/riemann-on-osv</id>
    <content type="html"><![CDATA[<p><strong>By Tzach Livyatan</strong></p>

<p>Clojure applications run on the JVM, so they&rsquo;re usually simple to run on OSv.  We have <a href="https://github.com/tzach/capstan-example-clojure">hello world in Clojure</a> running, but this time I wanted to port a real, non-toy, Clojure application. I chose <a href="http://riemann.io">Riemann</a>, a widely-used application for aggregating system events (and more).</p>

<p>I used <a href="http://osv.io/capstan/">Capstan</a>, a tool for building and running applications on OSv.  Jump to the end <a href="https://github.com/tzach/riemann">result</a>, or follow the steps I took:</p>

<!-- more -->


<p>Following the Capstan guideline, I added a <a href="https://github.com/tzach/riemann/blob/master/Capstanfile">Capstanfile</a> to the project.  Here are the parts of Capstanfile you need to know about:</p>

<ul>
<li>Set the base image. In this case I chose a base image with Java (open-jdk)</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>base: cloudius/osv-openjdk</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Build the jar file, taking advantage of the <code>lein uberjar</code> command, which packages the application with all dependencies into one jar file.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>build: lein uberjar</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Copy the build artifacts to the base image, producing a new image:</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>files:
</span><span class='line'>  /riemann.jar: ./target/riemann-0.2.5-SNAPSHOT-standalone.jar
</span><span class='line'>  /riemann.config: ./riemann.config</span></code></pre></td></tr></table></div></figure>


<p>I also copy the config file, which Riemann will look for.</p>

<ul>
<li>The run command for the VM is executed when the VM starts.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cmdline: /java.so -jar /riemann.jar</span></code></pre></td></tr></table></div></figure>


<p>That&rsquo;s it. Done with the Capstanfile.</p>

<p><strong>Let&rsquo;s test it!</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt;capstan run
</span><span class='line'>WARN [2014-04-13 14:11:22,029] Thread-9 - riemann.core - instrumentation service caught
</span><span class='line'>java.io.IOException: Cannot run program "hostname": error=0, vfork failed
</span><span class='line'>  at java.lang.ProcessBuilder.start(ProcessBuilder.java:1041)
</span><span class='line'>  at java.lang.Runtime.exec(Runtime.java:617)
</span><span class='line'>  at clojure.java.shell$sh.doInvoke(shell.clj:116)
</span><span class='line'>  at clojure.lang.RestFn.invoke(RestFn.java:408)</span></code></pre></td></tr></table></div></figure>


<p>No luck.  It turns out that Riemann is using</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>(sh "hostname")</span></code></pre></td></tr></table></div></figure>


<p>which uses vfork to run a child process. On any OS its not very efficient to fork just to get the hostname, and on current OSv it simply won&rsquo;t work. To bypass the problem, I replace this call with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>(.getHostName (java.net.InetAddress/getLocalHost))</span></code></pre></td></tr></table></div></figure>


<p>which uses a Java <code>getHostName</code>.</p>

<p><strong>Let&rsquo;s try again</strong></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>&gt;capstan run
</span></code></pre></td></tr></table></div></figure>


<p>This time it works, but how do I test it and connect to it?</p>

<p><strong>Let&rsquo;s use Capstan port forwarding</strong></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>capstan run -f 5555:5555 -f 5556:5556
</span></code></pre></td></tr></table></div></figure>


<p>
This will  forward host ports 5555 and 5556 to the corresponding ports on the OSv VM.</p>

<p><strong>Success :)</strong></p>

<p>Now we can switch to another terminal and run:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>riemann-health
</span></code></pre></td></tr></table></div></figure>


<p>
to generate traffic for Riemann
and</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>riemann-dash
</span></code></pre></td></tr></table></div></figure>


<p>to launch a Riemann web GUI.  Here is how it looks:</p>

<p><img src="http://osv.io/blog/images/riemann_on_osv.png" alt="&quot;Riemann GUI" /> <i>riemann-dash</i></p>

<p>Now we&rsquo;re ready to do further stress testing.  If you do find any problem, or have any question, you&rsquo;re invited to join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev list</a> and ask, or post an issue to the <a href="https://github.com/tzach/riemann">GitHub repository</a>.</p>

<p>&mdash; <a href="https://twitter.com/TzachL">Tzach Livyatan</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spinlock-free OS Design for Virtualization]]></title>
    <link href="http://osv.io/blog/blog/2014/04/19/spinlock-free/"/>
    <updated>2014-04-19T09:00:00-07:00</updated>
    <id>http://osv.io/blog/blog/2014/04/19/spinlock-free</id>
    <content type="html"><![CDATA[<p>Designing an OS to run specifically as a cloud guest doesn’t just mean stripping out features. There are some other important problems with running virtualized that a conventional guest OS doesn’t address.  In this post we&rsquo;ll cover one of them.</p>

<h2>Little spinlocks, big problem</h2>

<p>In any situation where code running on multiple CPUs might read or write the  same data, typical SMP operating systems use spinlocks. One CPU acquires the lock using an atomic test-and-set operation, and other CPUs that need the data must execute a busy-loop until they can acquire the lock themselves. Can I have the data? No. Can I have the data? No. Can I have the data? No. When an OS runs on bare hardware, a spinlock might just waste a little electricity. OS developers often use other more sophisticated locking techniques where they can, and try to reserve spinlocks for short-term locking of critical items.</p>

<p><img src="http://osv.io/blog/images/apachecon.jpg" alt="OSv hacking at Apachecon 2014" /> <i>Getting some high-performance web applications running on OSv at ApacheCON 2014</i></p>

<p>The problem comes in when you add virtualization. A physical CPU that holds a spinlock is actually working. The other CPUs in the system, “spinning” away waiting for the lock, are at least waiting for something that’s actually in progress. On a well-designed OS, the lock holder will be done quickly. When the OS is running under virtualization, though, it’s another story. The hypervisor might pause a virtual CPU at times when the guest OS can’t predict. As <a href="http://www.betriebssysteme.org/Aktivitaeten/Treffen/2008-Garching/Programm/docs/Abstract_Friebel.pdf">Thomas Friebel and Sebastian Biemueller described</a> (PDF) in “How to Deal with Lock Holder Preemption”,</p>

<blockquote><p>Lock holder preemption describes the situation when a VCPU is preempted inside the guest kernel while holding a spinlock. As this lock stays acquired during the preemption any other VCPUs of the same guest trying to acquire this lock will have to wait until the VCPU is executed again and releases the lock. Lock holder preemption is possible if two or more VCPUs run on a single CPU concurrently. And the more VCPUs of a guest are running in parallel the more VCPUs have to wait if trying to acquire a preempted lock. And as spinlocks imply active waiting the CPU time of waiting VCPUs is simply wasted.</p></blockquote>

<p>If the hypervisor pauses a virtual CPU while that VCPU holds a spinlock, you get into the bad situation where other virtual CPUs on your guest are just spinning, and it’s possible that no useful work is getting done in that guest&mdash;just electricity wasting. Friebel and Biemueller describe a solution to the problem involving a hypercall to complain about the wait. But the OSv solution to the problem is to remove spinlocks from the guest OS entirely.</p>

<h2>Why going spinlock-free matters</h2>

<p>As a first step, OSv does almost all of its kernel-level work in threads. Threads, which are allowed to sleep, can use lock-based algorithms. They use mutexes, not spinlocks, to protect shared data. The <a href="https://github.com/cloudius-systems/osv/blob/master/include/lockfree/mutex.hh">mutex implementation itself</a>, however, has to use a lock-free algorithm. OSv’s <a href="https://github.com/cloudius-systems/osv/blob/master/include/lockfree/mutex.hh">mutex implementation</a> is based on a lock-free design by Gidenstam &amp; Papatriantafilou, covered in <a href="http://domino.mpi-inf.mpg.de/internet/reports.nsf/c125634c000710d0c12560400034f45a/77c097efde9fa63fc125736800444203/$FILE/MPI-I-2007-1-003.pdf">LFTHREADS: A lock-free thread library.</a> (PDF).</p>

<p>One other place that can’t run as a thread, because it has to handle the low-level switching among threads, is the scheduler. The scheduler uses per-cpu run queues, so that almost all scheduling operations do not require coordination among CPUs, and lock-free algorithms when a thread must be moved from one CPU to another.</p>

<p>Lock-free design is just one example of the kind of thing that we mean when talking about how OSv is “designed for the cloud”.  Because we can’t assume that a CPU is always running or available to run, the low-level design of the OS needs to be cloud-aware to prevent performance degradation and resource waste.</p>

<p>We’ve been posting benchmarks that show sizeable performance increases running memcached and other programs. If you’re curious about whether OSv can make your application faster, please try it out from the <a href="http://osv.io/">OSv home page</a> or join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a>.</p>
]]></content>
  </entry>
  
</feed>
